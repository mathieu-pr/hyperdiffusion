defaults:
  - model: ae
  - _self_



# ───────── model ───────── #
model:
  input_dim: 36737

# ───────── global flags ───────── #
run_name: debug_run
device: cpu
seed: 42

# ───────── logging ─────────── #
logging:
  project: mlp_autoencoders
  log_dir: ${hydra:runtime.cwd}/autoencoders/logs

# ───────── optimiser ───────── #
optimizer:
  lr: 1e-3
  wd: 1e-5

# ───────── trainer ─────────── #
trainer:
  model_name: ae
  batch_size: 32
  ckpt_dir: ${hydra:runtime.cwd}/autoencoders/checkpoints   # where files land
  monitor: val/psnr                            # scalar name to watch
  mode: max                                    # "min" for losses, "max" for scores
  max_epochs: 5



dataset:
  root: ${hydra:runtime.cwd}/mlp_weights/Plane
  model_dims: [256, 256]
  mlp_kwargs: {}
  val_split: 0.10
  test_split: 0.10
  num_workers: 2

  filter_bad: false   
  augment: none               # or "permute", "sort_permute", etc.
  jitter_augment: false

  transformer_config:
    params:
      condition: false