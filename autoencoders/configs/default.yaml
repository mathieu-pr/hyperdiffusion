defaults:
  - model: ae
  - _self_



# ───────── model ───────── #
model:
  input_dim: 36737

# ───────── global flags ───────── #
run_name: full_latent512_hd8192_4096_2048
device: cpu
seed: 42

# ───────── logging ─────────── #
logging:
  project: mlp_autoencoders
  log_dir: ${hydra:runtime.cwd}/autoencoders/logs

# ───────── optimiser ───────── #
optimizer:
  lr: 1e-3
  wd: 1e-5

# ───────── trainer ─────────── #
trainer:
  model_name: ae
  batch_size: 32
  ckpt_dir: ${hydra:runtime.cwd}/autoencoders/checkpoints   # where files land
  monitor: val_recon_epoch                            # scalar name to watch
  mode: min                                    # "min" for losses, "max" for scores
  max_epochs: 150



dataset:
  root: ${hydra:runtime.cwd}/mlp_weights/Plane/3d_128_plane_multires_4_manifoldplus_slower_no_clipgrad
  # root: ${hydra:runtime.cwd}/mlp_weights/Plane/five_samples
  model_dims: [256, 256]
  mlp_kwargs: {}
  val_split: 0.15
  test_split: 0.15
  num_workers: 2

  filter_bad: false   
  augment: none               # or "permute", "sort_permute", etc.
  jitter_augment: false

  transformer_config:
    params:
      condition: false